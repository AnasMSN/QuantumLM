{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Custom Model Evaluation ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected keyword arguments: `padding`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Custom Model Evaluation ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Run BERTScore\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m bertscore \u001b[38;5;241m=\u001b[39m \u001b[43mBERTScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m bert_score \u001b[38;5;241m=\u001b[39m bertscore(preds, target)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Extract and compute mean of F1, precision, and recall\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torchmetrics/text/bert.py:158\u001b[0m, in \u001b[0;36mBERTScore.__init__\u001b[0;34m(self, model_name_or_path, num_layers, all_layers, model, user_tokenizer, user_forward_fn, verbose, idf, device, max_length, batch_size, num_threads, return_hash, lang, rescale_with_baseline, baseline_path, baseline_url, truncation, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     model_name_or_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name_or_path \u001b[38;5;241m=\u001b[39m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m _DEFAULT_MODEL\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m=\u001b[39m num_layers\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torchmetrics/metric.py:154\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    153\u001b[0m     kwargs_ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(kwargs)]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwargs_)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected keyword arguments: `padding`"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "from torchmetrics.text import BLEUScore, CharErrorRate, MatchErrorRate, WordErrorRate\n",
    "\n",
    "# Step 1: Load predictions and targets from JSON file\n",
    "with open(\"extracted_testcases.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# # Optional: remove <think> tags if desired\n",
    "# def clean(text):\n",
    "#     return text.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
    "\n",
    "# preds = [clean(d['generated']) for d in data]\n",
    "# target = [clean(d['ground_truth']) for d in data]\n",
    "preds = [d['generated'] for d in data]\n",
    "target = [d['ground_truth'] for d in data]\n",
    "\n",
    "# Step 2: Compute metrics\n",
    "print(\"=== Custom Model Evaluation ===\")\n",
    "\n",
    "# Run BERTScore\n",
    "bertscore = BERTScore()\n",
    "bert_score = bertscore(preds, target)\n",
    "\n",
    "# Extract and compute mean of F1, precision, and recall\n",
    "avg_f1 = bert_score['f1'].mean().item()\n",
    "avg_precision = bert_score['precision'].mean().item()\n",
    "avg_recall = bert_score['recall'].mean().item()\n",
    "\n",
    "# Print detailed scores\n",
    "print(\"All BERTScore F1:\", bert_score['f1'])\n",
    "print(f\"Mean F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Mean Precision: {avg_precision:.4f}\")\n",
    "print(f\"Mean Recall: {avg_recall:.4f}\")\n",
    "\n",
    "bleu1 = BLEUScore(n_gram=1)\n",
    "# print blue1 score\n",
    "print(\"BLEU1 score: \", bleu1(preds, target))\n",
    "\n",
    "bleu2 = BLEUScore(n_gram=2)\n",
    "# print blue2 score\n",
    "print(\"BLEU2 score: \", bleu2(preds, target))\n",
    "\n",
    "\n",
    "cer = CharErrorRate()\n",
    "print(\"CER score: \", cer(preds, target))\n",
    "\n",
    "mer = MatchErrorRate()\n",
    "print(\"MER score: \", mer(preds, target))\n",
    "\n",
    "wer = WordErrorRate()\n",
    "print(\"WER score: \", wer(preds, target))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfinetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
