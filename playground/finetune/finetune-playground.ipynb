{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from PIL import Image\n",
    "from transformers import TrainerCallback, LlavaNextImageProcessor\n",
    "from unsloth import FastVisionModel \n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class FinetuneQwenVL:\n",
    "    def __init__(self, \n",
    "                 data,\n",
    "                 epochs=1, \n",
    "                 learning_rate=1e-4,\n",
    "                 warmup_ratio=0.1,\n",
    "                 gradient_accumulation_steps=64,\n",
    "                 optim=\"adamw_torch\",\n",
    "                 model_id=\"unsloth/Qwen2-VL-7B-Instruct\", \n",
    "                 peft_r=8,\n",
    "                 peft_alpha=16,\n",
    "                 peft_dropout=0.05,\n",
    "                ):\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_id = model_id\n",
    "\n",
    "        self.base_model, self.tokenizer = FastVisionModel.from_pretrained(\n",
    "            model_name = self.model_id,\n",
    "            load_in_4bit = False,\n",
    "            use_gradient_checkpointing = \"unsloth\",\n",
    "        )\n",
    "        self.model = FastVisionModel.get_peft_model(\n",
    "            self.base_model,\n",
    "            finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "            finetune_language_layers   = True, # False if not finetuning language layers\n",
    "            finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "            finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "            r = peft_r,           \n",
    "            lora_alpha = peft_alpha,  \n",
    "            lora_dropout = peft_dropout,\n",
    "            bias = \"none\",\n",
    "            random_state = 3407,\n",
    "            use_rslora = False,  \n",
    "            loftq_config = None\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.optim = optim\n",
    "        self.data = data\n",
    "    \n",
    "    def format_data(self, row):\n",
    "        image_processor = LlavaNextImageProcessor.from_pretrained(self.model_id) # Use the correct model name\n",
    "        \n",
    "        \n",
    "        image_path = \"../\" + row[\"image\"]\n",
    "        input_text = row['input']\n",
    "        output_text = row['output']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = image.resize((336, 336))\n",
    "            processed_images = image_processor(images=image, return_tensors=\"pt\") \n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Unable to load image at path: {image_path}. Error: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": input_text,\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": processed_images.pixel_values[0],  \n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": output_text,\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the fine-tuning process.\n",
    "        \"\"\"\n",
    "        converted_dataset = [self.format_data(row) for row in self.data]\n",
    "        converted_dataset = converted_dataset\n",
    "        training_args = SFTConfig(\n",
    "            learning_rate=self.learning_rate,\n",
    "            output_dir='../model_cp_qutip_gs',\n",
    "            optim=self.optim,\n",
    "            logging_steps=1,\n",
    "            report_to=\"none\",\n",
    "            fp16 = not is_bf16_supported(),\n",
    "            bf16 = is_bf16_supported(),\n",
    "            logging_first_step=True,\n",
    "            warmup_ratio=self.warmup_ratio,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            logging_dir='./logs',\n",
    "            gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.epochs,\n",
    "            weight_decay = 0.01,            # Regularization term for preventing overfitting\n",
    "            lr_scheduler_type = \"linear\",   # Chooses a linear learning rate decay\n",
    "            seed = 3407,\n",
    "            logging_strategy = \"steps\",\n",
    "            # load_best_model_at_end = True,\n",
    "            # You MUST put the below items for vision finetuning:\n",
    "            remove_unused_columns = False,\n",
    "            dataset_text_field = \"\",\n",
    "            dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "            dataset_num_proc = 4,\n",
    "            max_seq_length = 2048,\n",
    "        )\n",
    "        FastVisionModel.for_training(self.model)\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model = self.model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            data_collator = UnslothVisionDataCollator(self.model, self.tokenizer), # Must use!\n",
    "            train_dataset = converted_dataset,\n",
    "            args = training_args,\n",
    "        )\n",
    "        trainer.train(resume_from_checkpoint=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.11: Fast Llava_Next vision patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.04s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m\n\u001b[1;32m     26\u001b[0m     fine_tune_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: images[i],\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompts[i] \u001b[38;5;241m+\u001b[39m x_train[i],\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_train[i],\n\u001b[1;32m     30\u001b[0m     })\n\u001b[1;32m     32\u001b[0m finetuner \u001b[38;5;241m=\u001b[39m FinetuneQwenVL(\n\u001b[1;32m     33\u001b[0m     data\u001b[38;5;241m=\u001b[39mfine_tune_data,\n\u001b[1;32m     34\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     peft_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 101\u001b[0m, in \u001b[0;36mFinetuneQwenVL.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    Executes the fine-tuning process.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     converted_dataset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata]\n\u001b[1;32m    102\u001b[0m     converted_dataset \u001b[38;5;241m=\u001b[39m converted_dataset\n\u001b[1;32m    103\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m    104\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m    105\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model_cp_qutip_gs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m    129\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[1], line 66\u001b[0m, in \u001b[0;36mFinetuneQwenVL.format_data\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     64\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m336\u001b[39m, \u001b[38;5;241m336\u001b[39m))\n\u001b[0;32m---> 66\u001b[0m     processed_images \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load image at path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/models/llava_next/image_processing_llava_next.py:728\u001b[0m, in \u001b[0;36mLlavaNextImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, image_grid_pinpoints, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, do_convert_rgb, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    715\u001b[0m image_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_image_patches(\n\u001b[1;32m    716\u001b[0m     image,\n\u001b[1;32m    717\u001b[0m     image_grid_pinpoints,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    724\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    725\u001b[0m )\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# preprocess patches\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_patches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_center_crop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_center_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pixel_values)\n\u001b[1;32m    744\u001b[0m new_images\u001b[38;5;241m.\u001b[39mappend(pixel_values)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/models/llava_next/image_processing_llava_next.py:421\u001b[0m, in \u001b[0;36mLlavaNextImageProcessor._preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    418\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[0;32m--> 421\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrescale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[1;32m    424\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(\n\u001b[1;32m    425\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format\n\u001b[1;32m    426\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/image_processing_utils.py:76\u001b[0m, in \u001b[0;36mBaseImageProcessor.rescale\u001b[0;34m(self, image, scale, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrescale\u001b[39m(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     48\u001b[0m     image: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Rescale an image by a scale factor. image = image * scale.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m        `np.ndarray`: The rescaled image.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrescale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/image_transforms.py:129\u001b[0m, in \u001b[0;36mrescale\u001b[0;34m(image, scale, data_format, dtype, input_data_format)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image must be of type np.ndarray, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m rescaled_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m scale  \u001b[38;5;66;03m# Numpy type promotion has changed, so always upcast first\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     rescaled_image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(rescaled_image, data_format, input_data_format)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CSV_FILENAME = '../metadata-qutip-grayscale-updated.csv' \n",
    "data = pd.read_csv(CSV_FILENAME)\n",
    "\n",
    "required_columns = ['type', 'image', 'ground_truth', 'prompt']\n",
    "for column in required_columns:\n",
    "    if column not in data.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in the CSV file.\")\n",
    "\n",
    "# Shuffle the dataset with a controlled random state\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_data = train_data.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "x_train = train_data['type'][:]\n",
    "images = train_data['image'][:]\n",
    "y_train = train_data['ground_truth'][:]\n",
    "prompts = train_data['prompt'][:]\n",
    "\n",
    "fine_tune_data = []\n",
    "for i in range(len(x_train)):\n",
    "    # print the index for debugging\n",
    "    fine_tune_data.append({\n",
    "        \"image\": images[i],\n",
    "        \"input\": prompts[i] + x_train[i],\n",
    "        \"output\": y_train[i],\n",
    "    })\n",
    "\n",
    "finetuner = FinetuneQwenVL(\n",
    "    data=fine_tune_data,\n",
    "    epochs=10,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    # model_id=\"unsloth/Qwen2-VL-7B-Instruct\",\n",
    "    model_id=\"unsloth/llava-v1.6-mistral-7b-hf\",\n",
    "    peft_r=32,\n",
    "    peft_alpha=32,\n",
    "    peft_dropout=0.0,\n",
    ")\n",
    "\n",
    "finetuner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfinetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
