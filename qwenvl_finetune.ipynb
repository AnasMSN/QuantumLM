{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from PIL import Image\n",
    "from transformers import TrainerCallback\n",
    "from unsloth import FastVisionModel \n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "\n",
    "class FinetuneQwenVL:\n",
    "    def __init__(self, \n",
    "                 data,\n",
    "                 epochs=1, \n",
    "                 learning_rate=1e-4,\n",
    "                 warmup_ratio=0.1,\n",
    "                 gradient_accumulation_steps=64,\n",
    "                 optim=\"adamw_torch\",\n",
    "                 model_id=\"unsloth/Qwen2-VL-7B-Instruct\", \n",
    "                 peft_r=8,\n",
    "                 peft_alpha=16,\n",
    "                 peft_dropout=0.05,\n",
    "                ):\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_id = model_id\n",
    "\n",
    "        self.base_model, self.tokenizer = FastVisionModel.from_pretrained(\n",
    "            model_name = self.model_id,\n",
    "            load_in_4bit = False,\n",
    "            use_gradient_checkpointing = \"unsloth\",\n",
    "        )\n",
    "        self.model = FastVisionModel.get_peft_model(\n",
    "            self.base_model,\n",
    "            finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "            finetune_language_layers   = True, # False if not finetuning language layers\n",
    "            finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "            finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "            r = peft_r,           \n",
    "            lora_alpha = peft_alpha,  \n",
    "            lora_dropout = peft_dropout,\n",
    "            bias = \"none\",\n",
    "            random_state = 3407,\n",
    "            use_rslora = False,  \n",
    "            loftq_config = None\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.optim = optim\n",
    "        self.data = data\n",
    "    \n",
    "    def format_data(self, row):\n",
    "        image_path = row[\"image\"]\n",
    "        input_text = row['input']\n",
    "        output_text = row['output']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Unable to load image at path: {image_path}. Error: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": input_text,\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": image,  \n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": output_text,\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the fine-tuning process.\n",
    "        \"\"\"\n",
    "        converted_dataset = [self.format_data(row) for row in self.data]\n",
    "        converted_dataset = converted_dataset\n",
    "        training_args = SFTConfig(\n",
    "            learning_rate=self.learning_rate,\n",
    "            output_dir='./model_cp',\n",
    "            optim=self.optim,\n",
    "            logging_steps=1,\n",
    "            report_to=\"none\",\n",
    "            fp16 = not is_bf16_supported(),\n",
    "            bf16 = is_bf16_supported(),\n",
    "            logging_first_step=True,\n",
    "            warmup_ratio=self.warmup_ratio,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            logging_dir='./logs',\n",
    "            gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
    "            num_train_epochs=self.epochs,\n",
    "            weight_decay = 0.01,            # Regularization term for preventing overfitting\n",
    "            lr_scheduler_type = \"linear\",   # Chooses a linear learning rate decay\n",
    "            seed = 3407,\n",
    "            logging_strategy = \"steps\",\n",
    "            # load_best_model_at_end = True,\n",
    "            # You MUST put the below items for vision finetuning:\n",
    "            remove_unused_columns = False,\n",
    "            dataset_text_field = \"\",\n",
    "            dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "            dataset_num_proc = 4,\n",
    "            max_seq_length = 2048,\n",
    "        )\n",
    "        FastVisionModel.for_training(self.model)\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model = self.model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            data_collator = UnslothVisionDataCollator(self.model, self.tokenizer), # Must use!\n",
    "            train_dataset = converted_dataset,\n",
    "            args = training_args,\n",
    "        )\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2_Vl vision patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.visual` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 189 | Num Epochs = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 8 | Total steps = 690\n",
      " \"-____-\"     Number of trainable parameters = 28,950,528\n",
      "ðŸ¦¥ Unsloth needs about 1-3 minutes to load everything - please wait!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/690 00:39 < 3:45:11, 0.05 it/s, Epoch 0.13/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.758600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/unsloth/models/_utils.py:584: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  source = re.sub(\"([^\\.])nn\\.\", r\"\\1torch.nn.\", source)\n",
      "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/unsloth/models/_utils.py:847: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  \"self.rotary_emb = .+?\\)\", function,\n",
      "/home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/unsloth/models/_utils.py:947: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  \"self.rotary_emb = .+?\\)\", function,\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacity of 23.68 GiB of which 7.93 GiB is free. Process 372511 has 9.14 GiB memory in use. Process 377890 has 470.00 MiB memory in use. Including non-PyTorch memory, this process has 6.13 GiB memory in use. Of the allocated memory 5.62 GiB is allocated by PyTorch, and 177.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m\n\u001b[1;32m     55\u001b[0m finetuner \u001b[38;5;241m=\u001b[39m FinetuneQwenVL(\n\u001b[1;32m     56\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     57\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     peft_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Run the finetuning process\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 131\u001b[0m, in \u001b[0;36mFinetuneQwenVL.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m FastVisionModel\u001b[38;5;241m.\u001b[39mfor_training(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    124\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m    125\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    126\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[1;32m    130\u001b[0m )\n\u001b[0;32m--> 131\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:383\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/unsloth/models/_utils.py:1069\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1067\u001b[0m     )\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DISK/MiniCodeLLM/QuantumLM/unsloth_compiled_cache/unsloth_compiled_module_qwen2_vl.py:1179\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1162\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2VLForConditionalGeneration_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DISK/MiniCodeLLM/QuantumLM/unsloth_compiled_cache/unsloth_compiled_module_qwen2_vl.py:878\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **loss_kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mget_dtype())\n\u001b[0;32m--> 878\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m     n_image_tokens \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    880\u001b[0m     n_image_features \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1009\u001b[0m, in \u001b[0;36mQwen2VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1009\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m blk(hidden_states, cu_seqlens\u001b[38;5;241m=\u001b[39mcu_seqlens, rotary_pos_emb\u001b[38;5;241m=\u001b[39mrotary_pos_emb)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    492\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    493\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/unsloth_zoo/gradient_checkpointing.py:452\u001b[0m, in \u001b[0;36mUnslothCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39m_requires_gradient: ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 452\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM\u001b[38;5;241m.\u001b[39mwait_stream(EXTRA_STREAM)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:426\u001b[0m, in \u001b[0;36mQwen2VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, cu_seqlens, rotary_pos_emb) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 426\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/DISK/MiniCodeLLM/QuantumLM/unsloth_compiled_cache/unsloth_compiled_module_qwen2_vl.py:410\u001b[0m, in \u001b[0;36mVisionSdpaAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, cu_seqlens: torch\u001b[38;5;241m.\u001b[39mTensor, rotary_pos_emb: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    409\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVisionSdpaAttention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DISK/MiniCodeLLM/QuantumLM/unsloth_compiled_cache/unsloth_compiled_module_qwen2_vl.py:394\u001b[0m, in \u001b[0;36mVisionSdpaAttention_forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    392\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    393\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    396\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.00 GiB. GPU 0 has a total capacity of 23.68 GiB of which 7.93 GiB is free. Process 372511 has 9.14 GiB memory in use. Process 377890 has 470.00 MiB memory in use. Including non-PyTorch memory, this process has 6.13 GiB memory in use. Of the allocated memory 5.62 GiB is allocated by PyTorch, and 177.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "prompt = \"I want you to act as a quantum computer specialized in performing Groverâ€™s algorithm. I will type a circuit, and you will reply with what a quantum computer should output. I want you to only reply with the output in a dictionary that contains the top-30 probabilities and nothing else. Circuit:\"\n",
    "qasm = \"\"\"\n",
    "qreg q[1];\n",
    "creg c[1];\n",
    "sdg q[0];\n",
    "tdg q[0];\n",
    "z q[0];\n",
    "sxdg q[0];\n",
    "u(5.6895009903655875,0.7219691165931532,2.039946133576617) q[0];\n",
    "rx(5.1067509121076995) q[0];\n",
    "y q[0];\n",
    "id q[0];\n",
    "u2(5.492256483666354,4.262016973063137) q[0];\n",
    "y q[0];\n",
    "measure q[0] -> c[0];\n",
    "\"\"\"\n",
    "\n",
    "# get csv data and get specific column 'openqasm'\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "CSV_FILENAME = 'Dataset/Source1/quantum_circuits_3_qubit.csv' \n",
    "data = pd.read_csv(CSV_FILENAME)\n",
    "x_train = data['openqasm'][:]\n",
    "image_path_1 = data['image_path1'][:]\n",
    "image_path_2 = data['image_path2'][:]\n",
    "y_train = data['ground_truth'][:]\n",
    "\n",
    "data = [\n",
    "    # {\n",
    "    #     \"image\": \"./Dataset/Source1/images/q01_d010_s0000_latex.png\",\n",
    "    #     \"input\": prompt + qasm,\n",
    "    #     \"output\": \"{\\\"0\\\": 0.099609375, \\\"1\\\": 0.900390625}\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    data.append({\n",
    "        \"image\": image_path_1[i],\n",
    "        \"input\": prompt + x_train[i],\n",
    "        \"output\": y_train[i],\n",
    "    })\n",
    "    # data.append({\n",
    "    #     \"image\": image_path_2[i],\n",
    "    #     \"input\": prompt + x_train[i],\n",
    "    #     \"output\": y_train[i],\n",
    "    # })\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# Initialize the finetune class with the data\n",
    "finetuner = FinetuneQwenVL(\n",
    "    data=data,\n",
    "    epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    model_id=\"unsloth/Qwen2-VL-2B-Instruct\",\n",
    "    peft_r=16,\n",
    "    peft_alpha=32,\n",
    "    peft_dropout=0.0,\n",
    ")\n",
    "\n",
    "# Run the finetuning process\n",
    "finetuner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cqilab/anaconda3/envs/llmfinetune/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastVisionModel\n",
    "from PIL import Image\n",
    "\n",
    "# Globals for holding the loaded model and processor\n",
    "MODEL = None\n",
    "TOKENIZER = None\n",
    "\n",
    "\n",
    "def find_highest_checkpoint(checkpoint_dir: str) -> str:\n",
    "    checkpoints = [\n",
    "        d for d in os.listdir(checkpoint_dir)\n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(checkpoint_dir, d))\n",
    "    ]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "\n",
    "    # Sort by the numeric portion after \"checkpoint-\"\n",
    "    checkpoints_sorted = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    highest_checkpoint = checkpoints_sorted[-1]\n",
    "    return os.path.join(checkpoint_dir, highest_checkpoint)\n",
    "\n",
    "\n",
    "def initialize_model(model_id: str, checkpoint_root: str = \"./model_cp_qutip\"):\n",
    "    global MODEL, TOKENIZER\n",
    "\n",
    "    # If already loaded, just return\n",
    "    if MODEL is not None and TOKENIZER is not None:\n",
    "        return MODEL, TOKENIZER\n",
    "\n",
    "    adapter_path = find_highest_checkpoint(checkpoint_root)\n",
    "    print(f\"Highest checkpoint found: {adapter_path}\")\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name =  adapter_path,  # Trained model either locally or from huggingface\n",
    "        load_in_4bit = False,\n",
    "    )\n",
    "    print(\"Base model loaded.\")\n",
    "\n",
    "    # 2. Find highest checkpoint\n",
    "\n",
    "    MODEL = model.to(\"cuda\")\n",
    "    TOKENIZER = tokenizer\n",
    "\n",
    "    return MODEL, TOKENIZER\n",
    "\n",
    "\n",
    "def run_inference_qwenvl(image: Image.Image, user_input: str, temperature: float = 0.0, \n",
    "                        max_tokens: int = 500, model_id: str = \"unsloth/Qwen2-VL-7B-Instruct\") -> str:\n",
    "\n",
    "    model, tokenizer = initialize_model(model_id)\n",
    "    FastVisionModel.for_inference(model) \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_input\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    # Tokenize prompt using the built-in chat template\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens = False,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        use_cache=True,\n",
    "        temperature=temperature,\n",
    "        min_p=0.1\n",
    "    )\n",
    "    generate_ids = output_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest checkpoint found: ./model_cp/checkpoint-2480\n",
      "Loading base model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     30\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/Qwen2-VL-2B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference_qwenvl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# generate print for the number of the test case and the generated textn result of generated_text and ground_truth\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest case \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mrun_inference_qwenvl\u001b[0;34m(image, user_input, temperature, max_tokens, model_id)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_inference_qwenvl\u001b[39m(image: Image\u001b[38;5;241m.\u001b[39mImage, user_input: \u001b[38;5;28mstr\u001b[39m, temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \n\u001b[1;32m     51\u001b[0m                         max_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/Qwen2-VL-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     FastVisionModel\u001b[38;5;241m.\u001b[39mfor_inference(model) \n\u001b[1;32m     55\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     56\u001b[0m         {\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         }\n\u001b[1;32m     68\u001b[0m     ]\n",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(model_id, checkpoint_root)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHighest checkpoint found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading base model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastVisionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Trained model either locally or from huggingface\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBase model loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 2. Find highest checkpoint\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/unsloth/models/loader.py:458\u001b[0m, in \u001b[0;36mFastVisionModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_exact_model_name:\n\u001b[1;32m    456\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m get_model_name(model_name, load_in_4bit)\n\u001b[0;32m--> 458\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m was_disabled: enable_progress_bars()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1054\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1052\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1054\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1056\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/configuration_utils.py:591\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/configuration_utils.py:650\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/file_download.py:923\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1291\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmfinetune/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"I want you to act as a quantum computer specialized in performing Groverâ€™s algorithm. I will type a circuit, and you will reply with what a quantum computer should output. I want you to only reply with the output in a dictionary that contains the top-30 probabilities and nothing else. Circuit:\"\n",
    "# qasm = \"OPENQASM 2.0;\\ninclude \\\"qelib1.inc\\\";\\nqreg q[3];\\ncreg c[3];\\nh q[0];\\nch q[2],q[1];\\nmeasure q[0] -> c[0];\\nmeasure q[1] -> c[1];\\nmeasure q[2] -> c[2];\\\",1024,\\\"{\\n    \\\"001\\\": 0.490234375,\\n    \\\"000\\\": 0.509765625\\n}\"\n",
    "\n",
    "# get data from csv file\n",
    "import pandas as pd\n",
    "import random  \n",
    "\n",
    "\n",
    "CSV_FILENAME = 'Dataset/Source1/quantum_circuits_3_qubit_test.csv'\n",
    "data = pd.read_csv(CSV_FILENAME)\n",
    "x_test = data['openqasm'][:]\n",
    "image_path_1 = data['image_path1'][:]\n",
    "image_path_2 = data['image_path2'][:]\n",
    "ground_truth = data['ground_truth'][:]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    # generate random number between 0 and 1 to chooose between image_path_1 and image_path_2, import random\n",
    "    random_number = random.random()\n",
    "    if random_number < 0.5:\n",
    "        image = Image.open(image_path_1[i]).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_path_2[i]).convert(\"RGB\")\n",
    "        \n",
    "    image = image.resize((336, 336))\n",
    "    \n",
    "    # image = Image.open().convert(\"RGB\")\n",
    "    user_input = prompt + x_test[i]\n",
    "    temperature = 1.5\n",
    "    max_tokens = 500\n",
    "    model_id = \"unsloth/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "    generated_text = run_inference_qwenvl(image, user_input, temperature, max_tokens, model_id)\n",
    "    # generate print for the number of the test case and the generated textn result of generated_text and ground_truth\n",
    "    print(f\"Test case {i+1}\")\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "    print(f\"Ground truth: {ground_truth[i]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1\n",
      "Generated text: This is a coherent state with alpha equal to 1, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 4, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 2\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 5, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 3\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.7, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 4\n",
      "Generated text: This is a random state with density is 0.5, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.5, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 5\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 8, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 6\n",
      "Generated text: This is a thermal state with average number of photons is 9, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 7\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 3 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 7, number of qubits equal to 3 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 8\n",
      "Generated text: This is a random state with density is 0.7, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.1, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 9\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.3, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 10\n",
      "Generated text: This is a coherent state with alpha equal to 8, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 6, number of qubits equal to 4 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 11\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 5, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 12\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 13 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 13\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1.4142135623730951, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 14\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 2, number of qubits equal to 6 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 15\n",
      "Generated text: This is a thermal state with average number of photons is 8, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 6, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 16\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 7 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 4, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 17\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.3, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 18\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 19\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 5, number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 20\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 1.0, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 21\n",
      "Generated text: This is a random state with density is 0.4, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.1, number of qubits equal to 12 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 22\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 23\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 6, number of qubits equal to 13 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 24\n",
      "Generated text: This is a coherent state with alpha equal to 8, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 3, number of qubits equal to 7 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 25\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 4, number of qubits equal to 4 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 26\n",
      "Generated text: This is a cat state with alpha equal to 3, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 4, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 27\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 28\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 6, number of qubits equal to 2 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 29\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 7, number of qubits equal to 9 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 30\n",
      "Generated text: This is a random state with density is 0.4, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.3, number of qubits equal to 8 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 31\n",
      "Generated text: This is a cat state with alpha equal to 0.5, number of qubits equal to 5 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 0.5, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 32\n",
      "Generated text: This is a coherent state with alpha equal to 3, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 3, number of qubits equal to 18 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 33\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 5, number of qubits equal to 10 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 34\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 5, number of qubits equal to 8 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 35\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 5, number of qubits equal to 2 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 36\n",
      "Generated text: This is a thermal state with average number of photons is 6, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 6, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 37\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 5, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 38\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1.4142135623730951, number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 39\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 9 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 40\n",
      "Generated text: This is a cat state with alpha equal to 2, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 2, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 41\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "Ground truth: This is a thermal state with average number of photons is 6, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 42\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 2 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 43\n",
      "Generated text: This is a thermal state with average number of photons is 8, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 8, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 44\n",
      "Generated text: This is a thermal state with average number of photons is 4, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 45\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.9, number of qubits equal to 12 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 46\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 1.4142135623730951, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 47\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 5, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 48\n",
      "Generated text: This is a thermal state with average number of photons is 6, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 5, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 49\n",
      "Generated text: This is a fock state with average number of photons is 1, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 1, number of qubits equal to 7 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 50\n",
      "Generated text: This is a number state with number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 51\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 18 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 7, number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 52\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.4, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 53\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 10 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 54\n",
      "Generated text: This is a fock state with average number of photons is 6, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 6, number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 55\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 56\n",
      "Generated text: This is a coherent state with alpha equal to 3, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 3, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 57\n",
      "Generated text: This is a fock state with average number of photons is 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 5, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 58\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.1, number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 59\n",
      "Generated text: This is a cat state with alpha equal to 0.5, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 0.5, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 60\n",
      "Generated text: This is a number state with number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 3 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 61\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 7, number of qubits equal to 10 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 62\n",
      "Generated text: This is a number state with number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 12 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 63\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 64\n",
      "Generated text: This is a thermal state with average number of photons is 8, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 8, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 65\n",
      "Generated text: This is a fock state with average number of photons is 8, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 3, number of qubits equal to 8 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 66\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 6, number of qubits equal to 14 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 67\n",
      "Generated text: This is a fock state with average number of photons is 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 5, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 68\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 4, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 69\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 3, number of qubits equal to 4 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 70\n",
      "Generated text: This is a cat state with alpha equal to 4, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 3, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 71\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 6, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 72\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 7 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 5, number of qubits equal to 5 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 73\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 74\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 1.4142135623730951, number of qubits equal to 20 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 75\n",
      "Generated text: This is a thermal state with average number of photons is 4, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 17 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 76\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 0.5, number of qubits equal to 2 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 77\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 5, number of qubits equal to 4 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 78\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 79\n",
      "Generated text: This is a cat state with alpha equal to 8, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 80\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.3, number of qubits equal to 3 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 81\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 8, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 82\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 83\n",
      "Generated text: This is a fock state with average number of photons is 3, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 4, number of qubits equal to 13 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 84\n",
      "Generated text: This is a coherent state with alpha equal to 1, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 1, number of qubits equal to 9 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 85\n",
      "Generated text: This is a fock state with average number of photons is 3, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 4, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 86\n",
      "Generated text: This is a fock state with average number of photons is 1, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 1, number of qubits equal to 16 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 87\n",
      "Generated text: This is a fock state with average number of photons is 6, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 6, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 88\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.6, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 89\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 90\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 3, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 91\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 4, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 92\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1.4142135623730951, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 93\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 94\n",
      "Generated text: This is a coherent state with alpha equal to 8, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 8, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 95\n",
      "Generated text: This is a cat state with alpha equal to 0.5, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 0.5, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 96\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 3, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 97\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 14 in the linear space -5 to 5.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 98\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 2 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 99\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 6, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 100\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 5, number of qubits equal to 16 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 101\n",
      "Generated text: This is a fock state with average number of photons is 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 7, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 102\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 0.5, number of qubits equal to 9 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 103\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.2, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 104\n",
      "Generated text: This is a fock state with average number of photons is 2, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 2, number of qubits equal to 3 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 105\n",
      "Generated text: This is a fock state with average number of photons is 6, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 6, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 106\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 2, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 107\n",
      "Generated text: This is a random state with density is 0.5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.5, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 108\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.7, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 109\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 2, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 110\n",
      "Generated text: This is a coherent state with alpha equal to 8, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 8, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 111\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 8, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 112\n",
      "Generated text: This is a number state with number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 2 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 113\n",
      "Generated text: This is a thermal state with average number of photons is 7, number of qubits equal to 5 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 5, number of qubits equal to 3 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 114\n",
      "Generated text: This is a coherent state with alpha equal to 1, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 0.5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 115\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.9, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 116\n",
      "Generated text: This is a coherent state with alpha equal to 1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 1, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 117\n",
      "Generated text: This is a random state with density is 0.5, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.5, number of qubits equal to 10 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 118\n",
      "Generated text: This is a cat state with alpha equal to 6, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 6, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 119\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.2, number of qubits equal to 6 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 120\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 2 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 7, number of qubits equal to 3 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 121\n",
      "Generated text: This is a coherent state with alpha equal to 8, number of qubits equal to 5 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 8, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 122\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 1.0, number of qubits equal to 9 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 123\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 124\n",
      "Generated text: This is a fock state with average number of photons is 8, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 3, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 125\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 3, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 126\n",
      "Generated text: This is a thermal state with average number of photons is 7, number of qubits equal to 5 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 6, number of qubits equal to 3 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 127\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 1, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 128\n",
      "Generated text: This is a number state with number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 10 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 129\n",
      "Generated text: This is a thermal state with average number of photons is 4, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 130\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.8, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 131\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 4, number of qubits equal to 8 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 132\n",
      "Generated text: This is a thermal state with average number of photons is 4, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 3, number of qubits equal to 9 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 133\n",
      "Generated text: This is a fock state with average number of photons is 7, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 8, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 134\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.6, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 135\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 10 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 136\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 2, number of qubits equal to 5 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 137\n",
      "Generated text: This is a random state with density is 0.4, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.2, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 138\n",
      "Generated text: This is a fock state with average number of photons is 6, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 6, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 139\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 7, number of qubits equal to 11 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 140\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.1, number of qubits equal to 14 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 141\n",
      "Generated text: This is a fock state with average number of photons is 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 5, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 142\n",
      "Generated text: This is a random state with density is 0.5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 143\n",
      "Generated text: This is a fock state with average number of photons is 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 7, number of qubits equal to 15 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 144\n",
      "Generated text: This is a cat state with alpha equal to 3, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 3, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 145\n",
      "Generated text: This is a cat state with alpha equal to 7, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 7, number of qubits equal to 16 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 146\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 4, number of qubits equal to 16 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 147\n",
      "Generated text: This is a number state with number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 13 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 148\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.7, number of qubits equal to 13 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 149\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 6, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 150\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 5, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 151\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 4, number of qubits equal to 16 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 152\n",
      "Generated text: This is a coherent state with alpha equal to 7, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 1.4142135623730951, number of qubits equal to 13 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 153\n",
      "Generated text: This is a thermal state with average number of photons is 2, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 1, number of qubits equal to 7 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 154\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 6 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 155\n",
      "Generated text: This is a thermal state with average number of photons is 8, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 8, number of qubits equal to 13 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 156\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 10 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.9, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 157\n",
      "Generated text: This is a random state with density is 0.9, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 0.2, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 158\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a thermal state with average number of photons is 4, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 159\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 160\n",
      "Generated text: This is a fock state with average number of photons is 6, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 6, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 161\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "Ground truth: This is a thermal state with average number of photons is 3, number of qubits equal to 5 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 162\n",
      "Generated text: This is a coherent state with alpha equal to 1, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 0.5, number of qubits equal to 12 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 163\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 2, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 164\n",
      "Generated text: This is a thermal state with average number of photons is 5, number of qubits equal to 14 in the linear space -5 to 5.\n",
      "Ground truth: This is a thermal state with average number of photons is 5, number of qubits equal to 14 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 165\n",
      "Generated text: This is a fock state with average number of photons is 7, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 8, number of qubits equal to 9 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 166\n",
      "Generated text: This is a random state with density is 0.1, number of qubits equal to 4 in the linear space -10 to 10.\n",
      "Ground truth: This is a random state with density is 1.0, number of qubits equal to 2 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 167\n",
      "Generated text: This is a coherent state with alpha equal to 5, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a coherent state with alpha equal to 5, number of qubits equal to 18 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 168\n",
      "Generated text: This is a number state with number of qubits equal to 17 in the linear space -10 to 10.\n",
      "Ground truth: This is a number state with number of qubits equal to 19 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 169\n",
      "Generated text: This is a fock state with average number of photons is 2, number of qubits equal to 15 in the linear space -10 to 10.\n",
      "Ground truth: This is a fock state with average number of photons is 2, number of qubits equal to 20 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 170\n",
      "Generated text: This is a cat state with alpha equal to 3, number of qubits equal to 8 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 7 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 171\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 7, number of qubits equal to 19 in the linear space -10 to 10.\n",
      "\n",
      "\n",
      "Test case 172\n",
      "Generated text: This is a cat state with alpha equal to 5, number of qubits equal to 11 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 1, number of qubits equal to 8 in the linear space -5 to 5.\n",
      "\n",
      "\n",
      "Test case 173\n",
      "Generated text: This is a cat state with alpha equal to 8, number of qubits equal to 12 in the linear space -10 to 10.\n",
      "Ground truth: This is a cat state with alpha equal to 8, number of qubits equal to 14 in the linear space -5 to 5.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from PIL import Image\n",
    "from transformers import TrainerCallback\n",
    "from unsloth import FastVisionModel \n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CSV_FILENAME = 'metadata-qutip-updated.csv' \n",
    "data = pd.read_csv(CSV_FILENAME)\n",
    "\n",
    "required_columns = ['type', 'image', 'ground_truth', 'prompt']\n",
    "for column in required_columns:\n",
    "    if column not in data.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in the CSV file.\")\n",
    "\n",
    "# Shuffle the dataset with a controlled random state\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "test_data = test_data.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "x_test = test_data['type'][:]\n",
    "images = test_data['image'][:]\n",
    "y_test = test_data['ground_truth'][:]\n",
    "prompts = test_data['prompt'][:]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    image = Image.open(images[i]).convert(\"RGB\").resize((336,336))\n",
    "    \n",
    "    user_input = \"What quantum state is depicted in this image?\" + x_test[i]\n",
    "    temperature = 0.5\n",
    "    max_tokens = 500\n",
    "    model_id = \"unsloth/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "    generated_text = run_inference_qwenvl(image, user_input, temperature, max_tokens, model_id)\n",
    "    # generate print for the number of the test case and the generated textn result of generated_text and ground_truth\n",
    "    print(f\"Test case {i+1}\")\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "    print(f\"Ground truth: {y_test[i]}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "# get len images,  x_train, y_train, prompts\n",
    "# print(f\"len(images): {len(images)}\")\n",
    "# print(f\"len(x_train): {len(x_train)}\")\n",
    "# print(f\"len(y_train): {len(y_train)}\")\n",
    "# print(f\"len(prompts): {len(prompts)}\")\n",
    "\n",
    "# print(images[16])\n",
    "\n",
    "# fine_tune_data = []\n",
    "# for i in range(len(x_train)):\n",
    "#     # print the index for debugging\n",
    "#     print(f\"Index: {i}\")\n",
    "#     fine_tune_data.append({\n",
    "#         \"image\": images[i],\n",
    "#         \"input\": prompts[i] + x_train[i],\n",
    "#         \"output\": y_train[i],\n",
    "#     })\n",
    "\n",
    "# finetuner = FinetuneQwenVL(\n",
    "#     data=fine_tune_data,\n",
    "#     epochs=10,\n",
    "#     learning_rate=5e-5,\n",
    "#     warmup_ratio=0.1,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     optim=\"adamw_torch_fused\",\n",
    "#     model_id=\"unsloth/Qwen2-VL-7B-Instruct\",\n",
    "#     peft_r=16,\n",
    "#     peft_alpha=16,\n",
    "#     peft_dropout=0.0,\n",
    "# )\n",
    "\n",
    "# finetuner.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfinetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
